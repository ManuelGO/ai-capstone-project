{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de6efed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from: /Users/marvelo/Documents/ImperialCollege/capstone/initial_data/function_4\n",
      "X shape: (30, 4)  | y shape: (30,)\n",
      "First row of X: [0.89698105 0.72562797 0.17540431 0.70169437]\n",
      "Shapes: (32, 4) (32,)\n",
      "Week 1 added -> [0.44739007 0.4067464  0.37675635 0.39856218] -0.0784261573095999\n",
      "Week 1 added -> [0.400828 0.42327  0.353541 0.436252] 0.48178123910921444\n",
      "Bounds usados:\n",
      " lb: [0. 0. 0. 0.] \n",
      " ub: [1. 1. 1. 1.]\n",
      "Kernel final: 1.01**2 * Matern(length_scale=[2.5, 2.06, 2.5, 2.5], nu=1.5) + WhiteKernel(noise_level=0.0001)\n",
      "Sugerencias Top-5 (EI xi=0.03):\n",
      " [[0.39992257 0.4814963  0.41761449 0.45510322]\n",
      " [0.37889415 0.47751338 0.428625   0.44675381]\n",
      " [0.40491767 0.48355283 0.40822442 0.44729766]\n",
      " [0.38883959 0.48119698 0.41954564 0.43277532]\n",
      " [0.39303284 0.47466474 0.43034456 0.44328637]]\n",
      "Guardado: /Users/marvelo/Documents/ImperialCollege/capstone/suggestions_f4_ei_w3.csv\n",
      "Submission format (Top-1):\n",
      "0.399923 - 0.481496 - 0.417614 - 0.455103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified upper bound 2.5. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 2 of parameter k1__k2__length_scale is close to the specified upper bound 2.5. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 3 of parameter k1__k2__length_scale is close to the specified upper bound 2.5. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 0.0001. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- CARGA Y PREP ---\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, qmc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "# from sklearn.ensemble import ExtraTreesRegressor  # <- alternativo robusto\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"initial_data\" / \"function_4\"\n",
    "assert DATA_DIR.exists(), f\"No se encontrÃ³ {DATA_DIR}\"\n",
    "# Datos\n",
    "X = np.load(DATA_DIR / \"initial_inputs.npy\", allow_pickle=True)\n",
    "y = np.load(DATA_DIR / \"initial_outputs.npy\", allow_pickle=True).ravel()\n",
    "X = np.asarray(X, float); \n",
    "y = np.asarray(y, float).ravel()\n",
    "print(\"Loaded from:\", DATA_DIR)\n",
    "print(\"X shape:\", X.shape, \" | y shape:\", y.shape)\n",
    "print(\"First row of X:\", X[0])\n",
    "# Semana 1\n",
    "x_w1 = np.array([\n",
    "0.44739007,0.4067464,0.37675635,0.39856218], dtype=float)\n",
    "y_w1 = float(-0.0784261573095999) \n",
    "\n",
    "x_w2 = np.array([0.400828, 0.42327 , 0.353541, 0.436252], dtype=float)\n",
    "y_w2 = float(0.48178123910921444)\n",
    "# Guardar los previos y actualizar\n",
    "X_prev, y_prev = X.copy(), y.copy()\n",
    "X = np.vstack([X, x_w1, x_w2])\n",
    "y = np.append(y, [y_w1, y_w2])\n",
    "n, d = X.shape\n",
    "print(\"Shapes:\", X.shape, y.shape)\n",
    "print(\"Week 1 added ->\", x_w1, y_w1)\n",
    "print(\"Week 1 added ->\", x_w2, y_w2)\n",
    "# Bounds desde datos + 10% acolchado, clamp a [0,1]\n",
    "lo, hi = X.min(axis=0), X.max(axis=0)\n",
    "pad = 0.10\n",
    "lb = np.clip(lo - pad*(hi - lo), 0.0, 1.0)\n",
    "ub = np.clip(hi + pad*(hi - lo), 0.0, 1.0)\n",
    "print(\"Bounds usados:\\n lb:\", lb, \"\\n ub:\", ub)\n",
    "# --- ESCALADO (solo X, una sola vez) ---\n",
    "sx = StandardScaler().fit(X)\n",
    "Xz = sx.transform(X)\n",
    "\n",
    "# --- GP ROBUSTO (F4: ruido alto) ---\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(d)*0.6, nu=1.5,\n",
    "                                      length_scale_bounds=(0.05, 2.5)) \\\n",
    "         + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-4, 1.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True,\n",
    "                              n_restarts_optimizer=18, alpha=1e-6, random_state=0)\n",
    "gp.fit(Xz, y)\n",
    "print(\"Kernel final:\", gp.kernel_)\n",
    "\n",
    "y_best = y.max()\n",
    "\n",
    "# --- Adquisiciones ---\n",
    "def acq_ucb(mu, sigma, kappa=1.6): return mu + kappa * sigma\n",
    "def acq_ei(mu, sigma, y_best, xi=0.03):\n",
    "    improve = mu - y_best - xi\n",
    "    Z = improve / (sigma + 1e-12)\n",
    "    return improve * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "def acq_var(sigma): return sigma\n",
    "\n",
    "def predict_on(gp, C, sx):\n",
    "    Cz = sx.transform(C)\n",
    "    mu, std = gp.predict(Cz, return_std=True)\n",
    "    return mu, np.clip(std, 1e-12, None)\n",
    "\n",
    "def too_close_Linf(c, X, tol=0.03):\n",
    "    return np.any(np.max(np.abs(X - c), axis=1) < tol)\n",
    "\n",
    "# --- CANDIDATOS: 80% TR + 20% Global ---\n",
    "best_idx = np.argmax(y)\n",
    "anchor = X[best_idx]\n",
    "L = 0.35  # TR moderada\n",
    "lb_tr = np.clip(anchor - 0.5*L*(ub - lb), 0, 1)\n",
    "ub_tr = np.clip(anchor + 0.5*L*(ub - lb), 0, 1)\n",
    "\n",
    "def lhs(lb, ub, n, seed):\n",
    "    U = qmc.LatinHypercube(d=len(lb), seed=seed).random(n); return lb + U*(ub - lb)\n",
    "\n",
    "C_tr = lhs(lb_tr, ub_tr, 50000, seed=4041)\n",
    "C_gl = lhs(lb,    ub,    12000, seed=4042)\n",
    "C = np.vstack([C_tr, C_gl])\n",
    "\n",
    "# Anti-borde\n",
    "edge_eps = 1e-3\n",
    "mask_edges = np.all((C > edge_eps) & (C < 1 - edge_eps), axis=1)\n",
    "C = C[mask_edges]\n",
    "\n",
    "# --- PREDICCIONES + EI(xi=0.03) ---\n",
    "muC, stdC = predict_on(gp, C, sx)\n",
    "ei = acq_ei(muC, stdC, y_best=y_best, xi=0.03)\n",
    "\n",
    "# --- SELECCIÃ“N Top-k evitando duplicados ---\n",
    "order = np.argsort(-ei)\n",
    "BATCH = 5\n",
    "S = []\n",
    "for idx in order:\n",
    "    c = C[idx]\n",
    "    if not too_close_Linf(c, X, tol=0.03):\n",
    "        S.append(c)\n",
    "        if len(S) == BATCH: break\n",
    "S = np.array(S)\n",
    "\n",
    "print(f\"Sugerencias Top-{BATCH} (EI xi=0.03):\\n\", S)\n",
    "out_path = BASE_DIR / \"suggestions_f4_ei_w3.csv\"\n",
    "pd.DataFrame(S, columns=[f\"x{i+1}\" for i in range(d)]).to_csv(out_path, index=False)\n",
    "print(\"Guardado:\", out_path)\n",
    "\n",
    "top1 = S[0]\n",
    "print(\"Submission format (Top-1):\")\n",
    "print(\" - \".join(f\"{v:.6f}\" for v in top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e368202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within [0,1]?  True\n",
      "Too close to existing (L_inf<0.02)?  False\n",
      "Submission:\n",
      "0.399923 - 0.481496 - 0.417614 - 0.455103\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cand = np.array([0.39992257, 0.4814963,  0.41761449, 0.45510322])\n",
    "def too_close_Linf(c, X, tol=0.02):\n",
    "    return np.any(np.max(np.abs(X - c), axis=1) < tol)\n",
    "\n",
    "print(\"Within [0,1]? \", np.all((cand >= 0) & (cand <= 1)))\n",
    "print(\"Too close to existing (L_inf<0.02)? \", too_close_Linf(cand, X, tol=0.02))\n",
    "print(\"Submission:\\n\" + \" - \".join(f\"{v:.6f}\" for v in cand))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c070e",
   "metadata": {},
   "source": [
    "BitÃ¡cora (2â€“3 lÃ­neas para F4)\n",
    "\tâ€¢\tMethod: GP (Matern Î½=1.5, normalize_y=True) + EI (Î¾=0.07), trust region alrededor de W1, anti-borde y anti-duplicado.\n",
    "\tâ€¢\tRationale: El Top-5 EI se concentrÃ³ cerca del punto de la semana 1; elegÃ­ el Top-1 para refinar localmente en una regiÃ³n prometedora pese al ruido.\n",
    "\tâ€¢\tSubmission: 0.400828 - 0.423270 - 0.353541 - 0.436252.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928cff8",
   "metadata": {},
   "source": [
    "ðŸ§¾ Function 4 â€“ Week 3 Log\n",
    "\n",
    "Update:\n",
    "This week, I added the second data point from Week 2, which showed a clear improvement over Week 1 (from â€“0.078 to +0.48). The dataset now includes 32 samples.\n",
    "\n",
    "Model setup:\n",
    "I kept a Gaussian Process with a Matern (Î½ = 1.5) kernel and added a ConstantKernel multiplier to better handle varying output scales. The model used normalize_y=True and WhiteKernel noise regularisation. The fitted kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ff209",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.01Â² Ã— Matern(length_scale=[2.5, 2.06, 2.5, 2.5]) + WhiteKernel(noise_level=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c97269",
   "metadata": {},
   "source": [
    "indicates a smooth, low-noise surfaceâ€”ideal for refining near the current optimum.\n",
    "\n",
    "Acquisition strategy:\n",
    "I applied Expected Improvement (EI) with a lower exploration parameter (Î¾ = 0.03), combining 80 % sampling inside a trust region centred on the best previous point and 20 % global candidates for robustness.\n",
    "This balances local exploitation with mild exploration.\n",
    "\n",
    "Result:\n",
    "The suggested next query was: 0.399923 â€“ 0.481496 â€“ 0.417614 â€“ 0.455103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e1795",
   "metadata": {},
   "source": [
    "which lies close to the prior high-performing region but shifts slightly along xâ‚‚ and xâ‚ƒ to probe a nearby ridge of potential improvement.\n",
    "\n",
    "Next steps:\n",
    "If this new evaluation confirms further gain, Iâ€™ll narrow Î¾ to 0.02 and maintain the same kernel.\n",
    "If performance stagnates, Iâ€™ll re-expand the trust region and re-tune noise bounds to encourage broader exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (imperial_venv)",
   "language": "python",
   "name": "imperial_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
